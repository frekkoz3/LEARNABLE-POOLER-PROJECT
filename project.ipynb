{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2410bc5f",
   "metadata": {},
   "source": [
    "# PROJECT\n",
    "\n",
    "This project is the final project of the academic course \"Introduction to Machine Learning\" (year 2024-2025).\n",
    "\n",
    "The aim of the project is to develop a new type of learnable (parameterized in a way that backpropagation can optimize it) pooler (Pooling Operation, eg MaxPool2d) for standard CNN architecture.\n",
    "\n",
    "The project is divided in 3 different parts:\n",
    "1. Literature review and existing poolers implementation in PyTorch\n",
    "2. Implementation of a new pooler in PyTorch\n",
    "3. Evaluation of the new pooler on CIFAR10, CIFAR100 and MNIST using a standard CNN\n",
    "\n",
    "The code will be develop in Python files, this notebook is the wrapper for the final product.\n",
    "\n",
    "Author : *Francesco Bredariol SM3201379*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4acaeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from poolers import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e20e45",
   "metadata": {},
   "source": [
    "## PART 1\n",
    "---\n",
    "*Implementing existing pooling operators.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f95a7a",
   "metadata": {},
   "source": [
    "### **Literature Review**\n",
    "\n",
    "MaxPooling and AvgPooling are the default poolers in the majority of cases, mostly because they are easy to implement, fast at training and at inference, really intuitive and perform state of the art on traditional datasets as CIFAR10, CIFAR100 and MNIST. It also to be said that, a rule of thumb, should always be \"keep it simple\" (if possible) and those two poolers just fit perfectly. By the way researchers started to notice that sometimes they can be not enough, and a lot of time they cut out important information during the downsampling phase. In addiction they are static, meaning that they can not learn any parameter to improve their performance. Given this in mind researchers developed others poolers, some of them being differentiable and learnable, some of them being static but clever.\n",
    "\n",
    "In order to obtain more information about existing and novels poolers I searched on the literature and ended up with two main papers. The first one is *\"A Comparison of Pooling Methods for Convolutional Neural Networks\"* (Zafar et al. 2022) while the second one is *\"Generalizing Pooling Functions in CNNs: Mixed, Gated, and Tree\"* (Lee et al. 2017). \n",
    "\n",
    "After studying them I decided to develop 4 different poolers:\n",
    "\n",
    "1. MixingPooling2d, a simple pooler that combines MaxPooling2d and AvgPooling2d with a learnable parameter (semi-static)\n",
    "2. GatedPooling2d, a pooler that combines MaxPooling2d and AvgPooling2s with a learnable gate (dynamic)\n",
    "3. StochasticPooling2d, a pooler that try to reduce overfitting introducing stochasticity inside the dunamic of pooling (static but stochastic)\n",
    "4. SP3Pooling2d, a novel pooler that aim to optimize the learning process introducing \"inner augmentation\" (static but stochastic)\n",
    "\n",
    "For MixingPooling2d and GatedPooling2d there were no need to find additional material. For StochasticPooling2d I retrieved its original paper *\"Stochastic Pooling for Regularization of Deep Convolutional Neural Networks\"* (Zeiler & Fergus, 2013) and the same for SP3Pooling2d *\"S3Pool: Pooling with Stochastic Spatial Sampling\"* (Zhai et al. 2017)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6478a8",
   "metadata": {},
   "source": [
    "### **MixingPooling2d**\n",
    "\n",
    "Given a pooling patch $R_j$, the output of the pooling function $S_j$ is\n",
    "\n",
    "$$\n",
    "    S_j = \\lambda \\max(a_i) + (1 - \\lambda)\\frac{1}{|R_j|}\\sum_{i\\in R_j}a_i\n",
    "$$\n",
    "\n",
    "In order to keep $\\lambda$ between 0 and 1, a sigmoid activation function is always applied to it, and in the end the equation can be rewritten as\n",
    "\n",
    "$$\n",
    "    S_j = \\sigma(\\lambda) \\max(a_i) + (1 - \\sigma(\\lambda))\\frac{1}{|R_j|}\\sum_{i\\in R_j}a_i\n",
    "$$\n",
    "\n",
    "where $\\sigma$ stands for the sigmoid function.\n",
    "\n",
    "$\\lambda$ is shared between all the patches in a channel.\n",
    "\n",
    "Results on how the backpropagation works are presented in the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e6100a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "tensor([0.5000], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[[[ 4.7500,  6.7500],\n",
      "          [12.7500, 14.7500]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.]]]])\n",
    "print(m)\n",
    "pooler = MixingPooling2d(2, 2, 1)\n",
    "print(pooler.get_core()) # This is set to 0 as default, in order to obtain a 0.5 mixing factor at the beginning\n",
    "print(pooler.forward(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8fbdc8",
   "metadata": {},
   "source": [
    "### **GatedPooling2d**\n",
    "\n",
    "Given a pooling patch $R_j$, the output of the pooling function $S_j$ is\n",
    "\n",
    "$$\n",
    "    S_j = \\sigma(\\omega^TR_j) \\max(a_i) + (1 -\\sigma(\\omega^TR_j))\\frac{1}{|R_j|}\\sum_{i\\in R_j}a_i\n",
    "$$\n",
    "\n",
    "where $\\sigma$ stands for the sigmoid function.\n",
    "\n",
    "The GatedPooling2d learn different weights for every part of the pooling patch and thanks to that can be more adaptive and dynamic than the MixingPooling2d. Those weights are called gating masks, since they can actually be learned by a convolutional mask applied on the pooling patch.\n",
    "\n",
    "While learning gating masks one has several options (listed in order of increasing number of parameters): learning one gating mask (a) per net, (b) per layer, (c) per layer/region being pooled (but used for all channels across that region), (d) per layer/channel (but used for all regions in each channel) (e) per layer/region/channel combination.\n",
    "\n",
    "My implementation is the (e). \n",
    "\n",
    "Results on how the backpropagation works are presented in the papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bebb731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0755,  0.3182],\n",
      "          [ 0.2447,  0.2119]]]], requires_grad=True)\n",
      "tensor([[[[ 5.9175,  7.9791],\n",
      "          [13.9997, 15.9999]]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.]]]])\n",
    "print(m)\n",
    "pooler = GatedPooling2d(2, 2, 1)\n",
    "print(pooler.get_core())\n",
    "print(pooler.forward(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6270161",
   "metadata": {},
   "source": [
    "### **StochasticPooling2d**\n",
    "\n",
    "Given a pooling patch $R_j$ we define a probability distribution on its indexes $\\pi(R_j)$.\n",
    "\n",
    "In the paper they use \n",
    "\n",
    "$$\n",
    "    p_i = \\frac{a_i}{\\sum_{k \\in R_j}a_k}\n",
    "$$\n",
    "\n",
    "Once $\\pi(\\cdot)$ is defined the output of the pooling function is\n",
    "\n",
    "$$\n",
    "    s_j = a_l \\text{ where } l \\sim Mult(1, \\pi(R_j))\n",
    "$$\n",
    "\n",
    "In short, activations are selected based on probabilities and further calculated by multinomial distribution.\n",
    "\n",
    "In my implementation I also developed $\\pi(\\cdot)$ as the softmax function and as the log_softmax function.\n",
    "\n",
    "This pooler has no learnable parameters.\n",
    "\n",
    "Results on how backpropagation works are presented in the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb507ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "tensor([[[[ 6.,  7.],\n",
      "          [14., 11.]]]])\n",
      "tensor([[[[ 5.,  8.],\n",
      "          [13., 15.]]]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.]]]])\n",
    "print(m)\n",
    "pooler = StochasticPooling2d(2, 2, 1, mode = \"softmax\")\n",
    "print(pooler.forward(m))\n",
    "pooler = StochasticPooling2d(2, 2, 1, mode = \"other\")\n",
    "print(pooler.forward(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452497c",
   "metadata": {},
   "source": [
    "### **SP3Pooling2d**\n",
    "\n",
    "SP3Pooling2d is the most novel poolers within the ones I implemented.\n",
    "\n",
    "SP3Pooling is a stochastic, grid-based downsampling method that selects a subset of spatial locations from an input feature map using structured random sampling. As the StochasticPooling2d it has no learnable parameters.\n",
    "\n",
    "It works as follows:\n",
    "\n",
    "1. Perform a MaxPooling2d with no dimensionality reduction\n",
    "2. Divide the input feature map into a grid of non-overlapping blocks (e.g., 4Ã—4 patches)\n",
    "3. Randomly sample a fixed number of rows and columns from each block (e.g., 2 out of 4)\n",
    "4. Select the elements at the intersections of the sampled rows and columns\n",
    "5. Repeat for each block, and combine all sampled patches into the downsampled output\n",
    "\n",
    "Since the formulas behind this is a bit tedious, for further details I suggest to read the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c92876ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "tensor([[[[11., 12.],\n",
      "          [15., 16.]]]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.]]]])\n",
    "print(m)\n",
    "pooler = SP3Pooling2d(2, 2, 1, device=\"cpu\")\n",
    "print(pooler.forward(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55286e",
   "metadata": {},
   "source": [
    "## PART 2\n",
    "---\n",
    "*Developing a new learnable stochastic pooling operators.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925137a",
   "metadata": {},
   "source": [
    "### **TDGSPooling2d**\n",
    "\n",
    "TDGS stands for \"Temperature Driven Gumbel SoftMax\".\\\n",
    "In this novel pooling operator the idea is to let a learnable temperature parameter $\\tau$ to model the Gumbel SoftMax (GSM).\\\n",
    "When $\\tau$ is low (<1) the GSM produces sharpened distribution centered in the max value (similarly to a MaxPool) while when $\\tau$$ is high (>1) the GSM produces softer distribution (similarly an uniform StochasticPool).\\\n",
    "The temperature parameter serves as balancer between deterministic behavior (which is somehow related to exploitation, MaxPool) and stochastic behavior (which is somehow related to exploration, Uniform StochasticPool).\\\n",
    "Developing a parameter per pooling patch (no temperature is shared) this can lead the architecture to learn specific spatial location where one strategy is better than the other.\n",
    "\n",
    "##### **Main Problem**\n",
    "\n",
    "What are the main challenges in developing this pooling function?\\\n",
    "We want to sample from a distribution parameterized by the parameter $\\tau$.\\\n",
    "The main problem is that it is not feasible to optimize the parameter of a probability distribution while sampling from it using backpropagation: it is non-differentiable.\n",
    "\n",
    "So, before diving in the solution, following this [tutorial](https://sassafras13.github.io/GumbelSoftmax/), let's dive in to some important aspects.\n",
    "\n",
    "##### **Reparameterization trick**\n",
    "\n",
    "Keep it simple: if we want to perform sampling and optimization from a gaussian, $z \\sim N(\\mu, \\sigma^2)$, how can we do?\\\n",
    "Well, short story short, math comes in help and rememeber us that \n",
    "$$\n",
    "    z = \\mu + \\sigma*\\epsilon \\text{ where } \\epsilon \\sim N(0, 1)\n",
    "$$\n",
    "This is perfect since now we can separate the deterministim (differentiable) from the stochasticity (non-differentiable). Visually this is what is happening:\n",
    "[reparameterization](/imgs/reparameterizationtrick.png)\n",
    "\n",
    "##### **Gumbel SoftMax**\n",
    "\n",
    "Our problem now is that we want to sample from a categorical distribution. How to achieve this? We do this by computing the log probabilities of all the classes in the distribution (deterministic) and adding them to some noise (stochastic) from a Gumbel distribution. Once we have combined the two parts we can compute the SoftMax, which is full differentiable, even adding a learnable temperature parameter. In my developement I computed the log probabilites of the input divided by the learnable temperature parameter since I found it more effective. Visually this is what is happening:\n",
    "[gumbelsoftmax](/imgs/gumbelsoftmax.png)\n",
    "\n",
    "A mathematical proof of why this actually works can be seen [here](https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/). \n",
    "\n",
    "##### **Bringing all together**\n",
    "\n",
    "One must notice that in order to obtain the desired behavior from this pooling technique something is missing: now we have a \"softmax\" as ouput, but we want just a raw single value, and we do not want to use a weighted sum. Luckily the Pytorch [implementation](https://docs.pytorch.org/docs/stable/generated/torch.nn.functional.gumbel_softmax.html) provides a \"hard\" return mode. If True, the returned samples will be discretized as one-hot vectors, but will be differentiated as if it is the soft sample in autograd. Using the one-hot vector now we can actually use the weighted sum as output, since it will just be the desired sampled value.\\\n",
    "And this is \"it\".\n",
    "\n",
    "Some little adjustments are also implemented.\n",
    "\n",
    "1. The temperature used while computing the log probabilities is not the raw temperature but a rectified one, to make it strictly positive\n",
    "2. A little epsilon is also added to the rectified temperature in order to prevent numerical errors\n",
    "3. Temperature is initialized from an Uniform distribution in $[0.5, 1.5]$ multiplied by an initial value\n",
    "4. During inference a weighted sum is performed as output (related to what done in StochasticPooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3992a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]], device='cuda:0')\n",
      "tensor([[[[ 6.,  8.],\n",
      "          [14., 16.]]]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[[[1., 2., 3., 4.], [5., 6., 7., 8.], [9., 10., 11., 12.], [13., 14., 15., 16.]]]]).to(\"cuda\")\n",
    "print(m)\n",
    "pooler = TDGSPooling2d(2, 2, 1, \"cuda\", 2, 2, initial_value=0)\n",
    "print(pooler.forward(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccab7a3",
   "metadata": {},
   "source": [
    "## PART 3\n",
    "---\n",
    "*Evaluating pooling operators.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc90ee3",
   "metadata": {},
   "source": [
    "Now we are dividing the logits by the temperature $t$ fixing $\\tau$ to 1. This is a little \"trick\", because what we are actually doing is: \n",
    "\n",
    "1. If $t<1$ we are gonna increase the relevance of the maximum (and so the noise added by the Gumbell(0, 1) impacts more)\n",
    "2. If $t>1$ we are gonna decrease the relevance of the maximum (and so the noise added by the Gumbell(0, 1) impacts less)\n",
    "\n",
    "This is great but $t$ has an asymmetric impact: we need a decrease in its value of only 0.9 (from 1 to 0.1) to obtain a \"full max\" behavior, while we nedd an increase in its value of 9 (from 1 to 10) to obtain a \"full uniform\" behavior, and this is the lower bound considering elements of the same magnitude.\n",
    "\n",
    "For now, to try to obtain elements always of the same magnitude, we are gonna normalize the vector before dividing it by the temperatures. This should not change the sensibility of the impact of $t$, but should help the overall sensibility.\n",
    "\n",
    "Setting the initial value is pretty important. Choosing 0 or 10 is usually a bad choice since they are bot too extreme. The initial value set to 1 should be fine, since it is close enough to zero but not to cloose to impose all the $t$ to become 0 (letting some stochasticity).\n",
    "\n",
    "After this adjustment think change pretty well, improving performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9467fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
